{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56c4350e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Abhinav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9036bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Abhinav\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.stem\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34fd1234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#string\n",
    "cricket=\"If the surprise test wasn’t bad enough, having the teacher announcing everyone’s marks the next day was. However, against all odds, it turns out that you have managed to do pretty decently. You strut around all day... the surprise test had yielded a surprising result.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e483b08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Abhinav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b82f17a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Abhinav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "25b8c0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Abhinav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77acc480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'the',\n",
       " 'surprise',\n",
       " 'test',\n",
       " 'wasn',\n",
       " '’',\n",
       " 't',\n",
       " 'bad',\n",
       " 'enough',\n",
       " ',',\n",
       " 'having',\n",
       " 'the',\n",
       " 'teacher',\n",
       " 'announcing',\n",
       " 'everyone',\n",
       " '’',\n",
       " 's',\n",
       " 'marks',\n",
       " 'the',\n",
       " 'next',\n",
       " 'day',\n",
       " 'was',\n",
       " '.',\n",
       " 'However',\n",
       " ',',\n",
       " 'against',\n",
       " 'all',\n",
       " 'odds',\n",
       " ',',\n",
       " 'it',\n",
       " 'turns',\n",
       " 'out',\n",
       " 'that',\n",
       " 'you',\n",
       " 'have',\n",
       " 'managed',\n",
       " 'to',\n",
       " 'do',\n",
       " 'pretty',\n",
       " 'decently',\n",
       " '.',\n",
       " 'You',\n",
       " 'strut',\n",
       " 'around',\n",
       " 'all',\n",
       " 'day',\n",
       " '...',\n",
       " 'the',\n",
       " 'surprise',\n",
       " 'test',\n",
       " 'had',\n",
       " 'yielded',\n",
       " 'a',\n",
       " 'surprising',\n",
       " 'result',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cricket_tokens=word_tokenize(cricket)\n",
    "cricket_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e3fe6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 56)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cricket_tokens),len(cricket_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "716e4df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist=FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "386667b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 4, ',': 3, '.': 3, 'surprise': 2, 'test': 2, '’': 2, 'day': 2, 'all': 2, 'If': 1, 'wasn': 1, ...})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in cricket_tokens:\n",
    "    fdist[i]=fdist[i]+1\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "143c788f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4),\n",
       " (',', 3),\n",
       " ('.', 3),\n",
       " ('surprise', 2),\n",
       " ('test', 2),\n",
       " ('’', 2),\n",
       " ('day', 2),\n",
       " ('all', 2),\n",
       " ('If', 1),\n",
       " ('wasn', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10=fdist.most_common(10)\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "247910ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "black_smoke=\"Did you know there was a tower,where they look out to the land,To see the people quickly passing by\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6199855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Did',\n",
       " 'you',\n",
       " 'know',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'tower',\n",
       " ',',\n",
       " 'where',\n",
       " 'they',\n",
       " 'look',\n",
       " 'out',\n",
       " 'to',\n",
       " 'the',\n",
       " 'land',\n",
       " ',',\n",
       " 'To',\n",
       " 'see',\n",
       " 'the',\n",
       " 'people',\n",
       " 'quickly',\n",
       " 'passing',\n",
       " 'by']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black_smoke_token = word_tokenize(black_smoke)\n",
    "black_smoke_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "316f3acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Did', 'you'),\n",
       " ('you', 'know'),\n",
       " ('know', 'there'),\n",
       " ('there', 'was'),\n",
       " ('was', 'a'),\n",
       " ('a', 'tower'),\n",
       " ('tower', ','),\n",
       " (',', 'where'),\n",
       " ('where', 'they'),\n",
       " ('they', 'look'),\n",
       " ('look', 'out'),\n",
       " ('out', 'to'),\n",
       " ('to', 'the'),\n",
       " ('the', 'land'),\n",
       " ('land', ','),\n",
       " (',', 'To'),\n",
       " ('To', 'see'),\n",
       " ('see', 'the'),\n",
       " ('the', 'people'),\n",
       " ('people', 'quickly'),\n",
       " ('quickly', 'passing'),\n",
       " ('passing', 'by')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.bigrams(black_smoke_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84c89ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Did', 'you', 'know'),\n",
       " ('you', 'know', 'there'),\n",
       " ('know', 'there', 'was'),\n",
       " ('there', 'was', 'a'),\n",
       " ('was', 'a', 'tower'),\n",
       " ('a', 'tower', ','),\n",
       " ('tower', ',', 'where'),\n",
       " (',', 'where', 'they'),\n",
       " ('where', 'they', 'look'),\n",
       " ('they', 'look', 'out'),\n",
       " ('look', 'out', 'to'),\n",
       " ('out', 'to', 'the'),\n",
       " ('to', 'the', 'land'),\n",
       " ('the', 'land', ','),\n",
       " ('land', ',', 'To'),\n",
       " (',', 'To', 'see'),\n",
       " ('To', 'see', 'the'),\n",
       " ('see', 'the', 'people'),\n",
       " ('the', 'people', 'quickly'),\n",
       " ('people', 'quickly', 'passing'),\n",
       " ('quickly', 'passing', 'by')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.trigrams(black_smoke_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3732009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Did', 'you', 'know', 'there'),\n",
       " ('you', 'know', 'there', 'was'),\n",
       " ('know', 'there', 'was', 'a'),\n",
       " ('there', 'was', 'a', 'tower'),\n",
       " ('was', 'a', 'tower', ','),\n",
       " ('a', 'tower', ',', 'where'),\n",
       " ('tower', ',', 'where', 'they'),\n",
       " (',', 'where', 'they', 'look'),\n",
       " ('where', 'they', 'look', 'out'),\n",
       " ('they', 'look', 'out', 'to'),\n",
       " ('look', 'out', 'to', 'the'),\n",
       " ('out', 'to', 'the', 'land'),\n",
       " ('to', 'the', 'land', ','),\n",
       " ('the', 'land', ',', 'To'),\n",
       " ('land', ',', 'To', 'see'),\n",
       " (',', 'To', 'see', 'the'),\n",
       " ('To', 'see', 'the', 'people'),\n",
       " ('see', 'the', 'people', 'quickly'),\n",
       " ('the', 'people', 'quickly', 'passing'),\n",
       " ('people', 'quickly', 'passing', 'by')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.ngrams(black_smoke_token,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5e67740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dba2d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('win', 'studi', 'buy')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"winning\"),pst.stem(\"studies\"),pst.stem(\"buying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "046d6543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5305c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_stem=[\"cats\",\"cacti\",\"geese\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e05eabd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats:cat\n",
      "cacti:cactus\n",
      "geese:goose\n"
     ]
    }
   ],
   "source": [
    "for i in words_to_stem:\n",
    "    print(i+\":\"+lemmatizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f87f3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos\n",
    "peace=\"What do you mean,'I dont believe in God'?I talk to him everyday.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da17164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peace_tokenize=word_tokenize(peace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f75b7374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('What', 'WP')]\n",
      "[('do', 'VB')]\n",
      "[('you', 'PRP')]\n",
      "[('mean', 'NN')]\n",
      "[(',', ',')]\n",
      "[(\"'\", \"''\")]\n",
      "[('I', 'PRP')]\n",
      "[('dont', 'NN')]\n",
      "[('believe', 'VB')]\n",
      "[('in', 'IN')]\n",
      "[('God', 'NNP')]\n",
      "[(\"'\", \"''\")]\n",
      "[('?', '.')]\n",
      "[('I', 'PRP')]\n",
      "[('talk', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('him', 'PRP')]\n",
      "[('everyday', 'NN')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for i in peace_tokenize:\n",
    "    print(nltk.pos_tag([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d998de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mary=\"Many had a little lamb,whom she really loved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f50738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mary_tokenize=word_tokenize(mary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ebc7bbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Many', 'had', 'a', 'little', 'lamb', ',', 'whom', 'she', 'really', 'loved']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mary_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82b6781d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Many', 'JJ')]\n",
      "[('had', 'VBD')]\n",
      "[('a', 'DT')]\n",
      "[('little', 'JJ')]\n",
      "[('lamb', 'NN')]\n",
      "[(',', ',')]\n",
      "[('whom', 'WP')]\n",
      "[('she', 'PRP')]\n",
      "[('really', 'RB')]\n",
      "[('loved', 'VBN')]\n"
     ]
    }
   ],
   "source": [
    "for i in mary_tokenize:\n",
    "    print(nltk.pos_tag([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3636936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75678a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "john=\"John lives in New York\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c6e82af",
   "metadata": {},
   "outputs": [],
   "source": [
    "john_token=word_tokenize(john)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f1b86d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'lives', 'in', 'New', 'York']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "john_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "92ce1791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'NNP'),\n",
       " ('lives', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('New', 'NNP'),\n",
       " ('York', 'NNP')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "john_tags=nltk.pos_tag(john_token)\n",
    "john_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01e9738f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (PERSON John/NNP) lives/VBZ in/IN (GPE New/NNP York/NNP))\n"
     ]
    }
   ],
   "source": [
    "john_ner=ne_chunk(john_tags)\n",
    "print(john_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a75a5ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.4.1-cp39-cp39-win_amd64.whl (11.8 MB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.4-cp39-cp39-win_amd64.whl (450 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.2-py3-none-any.whl (42 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.1-cp39-cp39-win_amd64.whl (1.3 MB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy) (1.20.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.7-cp39-cp39-win_amd64.whl (96 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4\n",
      "  Downloading pydantic-1.9.2-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
      "  Downloading spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp39-cp39-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.8-cp39-cp39-win_amd64.whl (18 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Collecting smart-open<6.0.0,>=5.2.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Collecting blis<0.10.0,>=0.7.8\n",
      "  Downloading blis-0.9.1-cp39-cp39-win_amd64.whl (7.4 MB)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.1-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\abhinav\\appdata\\roaming\\python\\python39\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: catalogue, srsly, pydantic, murmurhash, cymem, wasabi, typer, smart-open, preshed, confection, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "Successfully installed blis-0.9.1 catalogue-2.0.8 confection-0.0.1 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.8 pathy-0.6.2 preshed-3.0.7 pydantic-1.9.2 smart-open-5.2.1 spacy-3.4.1 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.4 thinc-8.1.1 typer-0.4.2 wasabi-0.10.1\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7be8812f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy-download\n",
      "  Downloading spacy_download-1.0.0-py3-none-any.whl (3.5 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spacy<4 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy-download) (3.4.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy<4->spacy-download) (8.1.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy<4->spacy-download) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy<4->spacy-download) (1.20.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy<4->spacy-download) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy<4->spacy-download) (2.4.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy<4->spacy-download) (1.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy<4->spacy-download) (3.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy<4->spacy-download) (1.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy<4->spacy-download) (4.62.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy<4->spacy-download) (2.26.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy<4->spacy-download) (58.0.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy<4->spacy-download) (0.6.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy<4->spacy-download) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy<4->spacy-download) (3.0.10)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy<4->spacy-download) (21.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy<4->spacy-download) (0.4.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy<4->spacy-download) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy<4->spacy-download) (1.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from spacy<4->spacy-download) (2.11.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<4->spacy-download) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<4->spacy-download) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<4->spacy-download) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4->spacy-download) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4->spacy-download) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4->spacy-download) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4->spacy-download) (2.0.4)\n",
      "Requirement already satisfied: blis<0.10.0,>=0.7.8 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<4->spacy-download) (0.9.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<4->spacy-download) (0.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\abhinav\\appdata\\roaming\\python\\python39\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<4->spacy-download) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<4->spacy-download) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\abhinav\\anaconda3\\lib\\site-packages (from jinja2->spacy<4->spacy-download) (1.1.1)\n",
      "Installing collected packages: spacy-download\n",
      "Successfully installed spacy-download-1.0.0\n"
     ]
    }
   ],
   "source": [
    "pip install spacy-download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a2fd211a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_21180/3004318277.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Abhinav\\AppData\\Local\\Temp/ipykernel_21180/3004318277.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    python -m spacy download en\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "42e0c77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "from spacy_download import load_spacy\n",
    "nlp=load_spacy(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b433df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "13131263",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(\"This is spartial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "924e7f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "spartial\n"
     ]
    }
   ],
   "source": [
    "#tokenisation\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dcfc6816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spartial"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token=doc[2]\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "101ce7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spartial"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span=doc[2:5]\n",
    "span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1bb5b6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 this\n",
      "1 is\n",
      "2 sparta\n",
      "3 !\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"this is sparta!\")\n",
    "for token in doc:\n",
    "    print(token.i,token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6f6a387f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joey doesn't share pizza!!"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=nlp(\"joey doesn't share pizza!!\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "54efe5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 joey PROPN\n",
      "1 does AUX\n",
      "2 n't PART\n",
      "3 share VERB\n",
      "4 pizza NOUN\n",
      "5 ! PUNCT\n",
      "6 ! PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.i,token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "13c723f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ner\n",
    "doc=nlp(\"Apple is looking to buy U.K. startup for 1$ billion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "395eb469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "1$ billion MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0d5f9d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(\"Barack Obama the former president of united states will be vacating white house today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4d9fee8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama PERSON\n",
      "united states ORG\n",
      "today DATE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0424a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matcher \n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c03bd137",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(\"Barack Obama the former president of united states will be vacating white house today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d0753124",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=[{'LEMMA':'vacant'},{'ORTH':'white'}]\n",
    "matcher=Matcher(nlp.vocab)\n",
    "matcher.add('white_pattern',[pattern])\n",
    "matches=matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dcf37dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for match_id,start,end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5f85b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(\"2018 FIFA world cup :france won!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bbc605ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns=[{'IS_DIGIT':True},{'LOWER':'fifa'},{'LOWER':'world'},{'LOWER':'cup'}]\n",
    "matcher2=Matcher(nlp.vocab)\n",
    "matcher2.add('fifa_pattern',[pattern])\n",
    "matches2=matcher2(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae43506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "32498b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for match_id,start,end in matches2:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b128b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
